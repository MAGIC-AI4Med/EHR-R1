# Toward Reasoning-Enhanced Large Langugage Models for EHR Analysis
[![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)]() 
[![Data](https://img.shields.io/badge/Physionet-Dataset-blue)](https://huggingface.co/datasets/QiaoyuZheng/DiagRL-Corpus)

[![Model](https://img.shields.io/badge/Huggingface-EHR--R1--1.7B-yellow)](https://huggingface.co/BlueZeros/EHR-R1-1.7B) 
[![Model](https://img.shields.io/badge/Huggingface-EHR--R1--8B-yellow)](https://huggingface.co/BlueZeros/EHR-R1-8B) 
[![Model](https://img.shields.io/badge/Huggingface-EHR--R1--72B-yellow)](https://huggingface.co/BlueZeros/EHR-R1-72B) 

We introduce a large-scale, high-quality dataset and a specialized, reasoning-enhanced LLM, **EHR-Ins**, contains **3.5M** of cases across **42** diverse EHR tasks and **300k** synthesized EHR reasoning chains. Building upon this dataset, we developed **EHR-R1** series, a family of LLMs up to 72B parameters, trained through a three-stage curriculum involving domain adaptation, reasoning enhancement, and reinforcement learning.

----------------------------------------
## ðŸ”– **TOC**
  * [**KeyPoint**](#--keypoint--)
  * [**Directly Use**](#--directly-use--)
    + [**EHR Input Format**](#ehr-input-format)
    + [**Models Inference**](#--models-inference--)
  * [**Quick Start**](#--quick-start--)
    + [**Step 1: Enviorment Install**](#--step-1--enviorment-install--)
    + [**Step 2: Preprocess Data**](#--step-2--preprocess-data--)
    + [**Step 2: Training LLMs**](#--step-2--training-llms--)
    + [**Step 3: Evaluation**](#--step-3--evaluation--)
  * [**Advanced Usage**](#--advanced-usage--)
    + [**Generate Custom Index File**](#generate-custom-index-file)
    + [**Generate Reasoning Data**](#generate-reasoning-data)
  * [**Citation**](#citation)
## ðŸ’¡ **KeyPoint**
* We open-source a large-scale instruction dataset [**EHR-Ins**](data_url), including 3.5M non-reasoning data and 300k reasoning data.

* We open-source a comprehensive benchmark [**EHR-Bench**](data_url), which covers 42 distinct EHR analysis tasks.

* We open-source EHR reasoning-enhanced LLMs **EHR-R1**, including [**EHR-R1-1.7B**](https://huggingface.co/BlueZeros/EHR-R1-1.7B), [**EHR-R1-8B**](https://huggingface.co/BlueZeros/EHR-R1-8B), and [**EHR-R1-72**](https://huggingface.co/BlueZeros/EHR-R1-72B)

* We open-source the "thinking-graph" pipeline, which can synthesize reasoning chains for EHR analysis tasks according to the relation of EHR entities.

<img src="./assets/teaser.png"/>

----------------------------------------

## âš¡ **Directly Use**
### **EHR Input Format**
For any EHR data, keep the EHR input with markdown format as below:
* For the event with signle record:
```markdown
## Evant Name [Event Time (YYYY-MM-DD HH:MM:SS)]
- ItemKey_1: ItemValue_1
- ItemKey_2: ItemValue_2
- ItemKey_3: ItemValue_3
```
* For the event with multiple records (like labevents):
```markdown
## Evant Name [Event Time (YYYY-MM-DD HH:MM:SS)]
 |  ItemKey_1  |  ItemKey_2  |  ItemKey_3  |
 |  ---------  |  ---------  |  ---------  |
 | ItemValue_1 | ItemValue_2 | ItemValue_3 |
 | ItemValue_1 | ItemValue_2 | ItemValue_3 |
 | ItemValue_1 | ItemValue_2 | ItemValue_3 |
```

### **Models Inference**
* Inference with **VLLM** (Recommand for faster decoding)
```python
from vllm import LLM, SamplingParams
model_name = "{Path to EHR-R1}"
model = LLM(
    model=model_name, 
    tensor_parallel_size=torch.cuda.device_count(), 
    trust_remote_code=True, 
    max_model_len=32000, 
    max_seq_len_to_capture=32000, 
    gpu_memory_utilization=0.7
)

ehr_input = "{YOUR FOMATTED EHR INPUT}"
instruction = "{YOUR TASK INSTRUCTION}"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": ehr_input + "\n" + instruction}
]

# For EHR-R1-1.7B & EHR-R1-8B, control the reasoning mode by setting enable_thinking
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False,
).to(model.device)
# For EHR-R1-72B, you can manually add <think>\n\n<\think>\n at the end of the model_inputs to close the reasoning modes.
text += "<think>\n\n</think>\n"

sampling_params = SamplingParams(
    temperature=0.0,
    max_tokens=2048
)

outputs = model.generate(
    text, 
    sampling_params=sampling_params, 
    use_tqdm=False
)
print(output)
```
> **Note**: You can control the reasoning mode of EHR-R1-1.7B and EHR-R1-8B by setting `enable_thinking` parameters. However, the tokenizer of the EHR-R1-72B miss the enable_thinking parameters and will enable the reasoning mode automatically. You can manually add `<think>\n\n<\think>\n` at the end of the `model_inputs` to close the reasoning modes.

* Inference with **Transformers**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
model_name = "{Path to EHR-R1}"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
ehr_input = "{YOUR FOMATTED EHR INPUT}"
instruction = "{YOUR TASK INSTRUCTION}"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": ehr_input + "\n" + instruction}
]

# For EHR-R1-1.7B & EHR-R1-8B, control the reasoning mode by setting enable_thinking
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False,
).to(model.device)
# For EHR-R1-72B, you can manually add <think>\n\n<\think>\n at the end of the model_inputs to close the reasoning modes.
text += "<think>\n\n</think>\n"

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048,
    temperautre=0.0
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

----------------------------------------

## ðŸ“™ **Quick Start**
### **Step 1: Enviorment Install**
```bash
pip install -r requirements.text
```

### **Step 2: Preprocess Data**
* **(Option 1)** Download Dataset from [Physionet](https://physionet.org/content/mimic-iv-ehr-analysis/1.0/). Put  into `./datas` and unzip all files. The directory is as follow:
```
mimic-iv-ehr-analysis/
â”œâ”€â”€ patients_ehr.tar.gz
â”œâ”€â”€ ehr_ins/
â”‚   â”œâ”€â”€ ehr_ins.csv
â”‚   â”œâ”€â”€ ehr_ins_reasoning.csv
â”‚   â””â”€â”€ ehr_ins_rl.csv
â”œâ”€â”€ ehr_bench/
â”‚   â”œâ”€â”€ ehr_bench_decision_making.csv
â”‚   â””â”€â”€ ehr_bench_risk_prediction.csv
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ item_set.tar.gz
â”‚   â””â”€â”€ similar_item.tar.gz
â””â”€â”€ index_mapping/
    â””â”€â”€ other index file...
```
* **(Option 2)** Preprocess from MIMIC-IV raw Dataset
```bash
python ./preprocess/merge_patient.py
python ./preprocess/rank.py
python ./preprocess/combination.py
```

### **Step 3: Training LLMs**
```bash
OUTPUT_ROOT="{YOUR_PATH_TO_SAVE_CHECKPOINT}"
LOG_ROOT="{YOUR_PATH_TO_LOG_TRAINING_PROCESS}"

DATA_PATH="{TRAINING_INDEX_FILE}"
DATA_NAME="{TRAINING_DATA_NAME}"

MODEL_PATH="{LLM_PATH}"
MODEL_NAME="{LLM_NAME}"

CKPT_NAME=${DATA_NAME}-${MODEL_NAME}

LOG_PATH=${LOG_ROOT}/${CKPT_NAME}
mkdir -p ${LOG_PATH}

accelerate launch --config_file=./scripts/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 \
    sft.py \
    --bf16 True \
    --use_liger_kernel \
    --accelerator_config='{"split_batches": true, "dispatch_batches": true}' \
    --load_dataset_mode "lazzy" \
    --model_name_or_path ${MODEL_NAME} \
    --dataset_name ${DATA_PATH} \
    --output_dir ${OUTPUT_ROOT}/${CKPT_NAME} \
    --max_seq_length 8192 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --eval_strategy "no" \
    --save_strategy "steps" \
    --save_steps 1000 \
    --save_total_limit 3 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --dataset_num_proc 1 \
    --dataloader_num_workers 8 \
    --dataloader_pin_memory \
    --ignore_data_skip \
    --report_to "wandb" 2>&1 | tee ${LOG_PATH}/train.log
```
You can find the scripts of three-stage training in `./scripts/train`.

### **Step 4: Evaluation**
You can directly evaluate the inference with the following command:
```bash
OUTPUT_ROOT="{YOUR_PATH_TO_SAVE_RESULTS}"

DATASET="{TRAINING_INDEX_FILE}"
DATA_NAME="{TRAINING_DATA_NAME}"

MODEL_PATH="{LLM_PATH}"
MODEL_NAME="{LLM_NAME}"

mkdir -p ${OUTPUT_ROOT}/${DATA_NAME}/${MODEL_NAME}
python test.py \
        --dataset_name ${DATASET} \
        --output_path ${OUTPUT_ROOT}/${DATA_NAME}/${MODEL_NAME} \
        --model_name_or_path ${MODEL_PATH} \
        --gpu_memory_utilization 0.85 \
        --max_seq_len 32000 \
        --use_vllm \ # use vllm for faster decoding
        --batch 1 \
        --prompt \ # enable prompt when evaluating the baseline models.
        --think_prompt \ # enable think_prompt when enable thinking mode for reasoning models
        --resume
```
We also provid advanced inference scripts in `./scripts/test`.

----------------------------------------

## ðŸ“ˆ **Advanced Usage**
### **1. Generate Custom Index File**
* First generate all index from the whole MIMIC-IV data.
```bash
DATASET=all
python mimiciv_dataset/task_sample_info_gen.py \
  --patient_id ./datas/patient_data/patients.csv \
  --output_path ./datas/task_index/${DATASET}
```
* Then sample the index with the config
```bash
DATASET=test # sample from train or test subset
DATA_CONFIG="risk_prediction" # config name
SAMPLE_NUM=500 # reset sample num over the data config

DATA_CONFIG_PATH="./scripts/data_configs/${DATA_CONFIG}.json"
python ./mimiciv_dataset/data_index_gen.py \
  --data_index_dir ./datas/task_index/all \
  --subject_id_path ./datas/patient_data/${DATASET}.csv \
  --data_config ${DATA_CONFIG_PATH} \
  --output_path ./datas/task_index/${DATA_CONFIG}/${DATASET}_${DATA_CONFIG}_${SAMPLE_NUM}.csv \
  --force_task_num ${SAMPLE_NUM} \
  --balance_force # whether to adopt the label-wise weighted sampling
```
> **Note:** You can set the data config in the format as below:
> ```json
> {
>   "task_type": task_num 
> }
> ```
> Example configs are shown in `./scripts/data_configs`.

### **2. Generate Reasoning Data**
* **Step1: Generate task index grouped by patient**
```bash
DATASET=patient
python mimiciv_dataset/task_sample_info_gen.py \
  --patient_id ./datas/patient_data/patients.csv \
  --output_path ./datas/task_index/${DATASET}
  --group patient
```
* **Step2: Install [QuickUMLS](https://github.com/Georgetown-IR-Lab/QuickUMLS)**
* **Step3: Extract medical entities from EHR data.**
```bash
export NLTK_DATA={YOUR_PATH_TO_NLTK}
export QUICKUMLS={YOUR_PATH_TO_QUICKUMLS_FILE}

python ./thinking_graph_preprocess/concept_extraction.py
```
* **Step4: Statistic co-exist metrix of medical entities.**
```bash
python thinking_graph_preprocess/coexistence_concept.py \
    --data_index_dir "./datas/task_index/all" \
    --subject_id_path "./datas/patient_data/train.csv"  \
    --patient_ehr_path "./datas/patients_ehr" \
    --output_path "./datas/evidence_datas"
```
* **Step5: Gather thinking-graph for samples**
```bash
WORK_NUM=10
DATA_DIR={YOUR_INDEX_FILE}

# run multiple programs to speed up
for WORK_IDX in $(seq 0 $((WORK_NUM-1)))
do
    echo ${WORK_IDX}
    python ./thinking_graph_preprocess/evidence_generation.py \
        --data_index_path ${DATA_DIR} \
        --chunk_num ${WORK_NUM} \
        --chunk_idx ${WORK_IDX} \
        --threshold 5 &
done
```
* **step6: Synthesize reasoning chain for EHR taks**
```bash
DATA_INDEX_DIR={YOUR_EVIDECNCE_FILE}

python ./thinking_graph_preprocess/reasoning_generation.py \
    --data_index_dir ${DATA_INDEX_DIR} \
    --filter_nograph \
    --without_knowledge \
    --model "gpt-4o" \
    --num_worker 20
```

## **Citation**
```bib

```